{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Reinforcement Learning Pipeline\n",
        "\n",
        "This notebook shows a clean, minimal RL pipeline for dynamic car pricing using the local `CarPricingEnv`.\n",
        "It uses **tabular Q-learning** (no deep learning required) so each step is easy to understand and modify.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Imports and reproducibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from env import CarPricingEnv, build_constraint_fn, default_terminal_reward\n",
        "\n",
        "rng = np.random.default_rng(7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Define a small environment setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthetic baseline data for one item/listing\n",
        "x: Dict = {\"p0\": 50000.0}\n",
        "\n",
        "def p0_fn(features: Dict) -> float:\n",
        "    return float(features[\"p0\"])\n",
        "\n",
        "def hazard_fn(features: Dict, t: int, delta: float) -> float:\n",
        "    # Lower prices (negative delta) should sell faster; time pressure increases conversion.\n",
        "    logits = -2.2 - 1.4 * delta + 0.12 * t\n",
        "    return float(1 / (1 + np.exp(-logits)))\n",
        "\n",
        "constraint_fn = build_constraint_fn(delta_min=-0.25, delta_max=0.25)\n",
        "holding_cost = 60.0\n",
        "t_max = 25\n",
        "\n",
        "env = CarPricingEnv(\n",
        "    hazard_fn=hazard_fn,\n",
        "    p0_fn=p0_fn,\n",
        "    constraint_fn=constraint_fn,\n",
        "    holding_cost=holding_cost,\n",
        "    t_max=t_max,\n",
        "    rng=rng,\n",
        "    terminal_reward_fn=default_terminal_reward,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Build a tiny tabular Q-learning agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "delta_grid = np.round(np.linspace(-0.25, 0.25, 21), 3)\n",
        "action_set = np.array([-0.03, -0.01, 0.0, 0.01, 0.03], dtype=float)\n",
        "\n",
        "state_index = {(t, float(d)): i for i, (t, d) in enumerate((t, d) for t in range(1, t_max + 1) for d in delta_grid)}\n",
        "q_table = np.zeros((len(state_index), len(action_set)), dtype=float)\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    episodes: int = 4000\n",
        "    alpha: float = 0.08\n",
        "    gamma: float = 0.98\n",
        "    eps_start: float = 1.0\n",
        "    eps_end: float = 0.05\n",
        "\n",
        "cfg = TrainConfig()\n",
        "\n",
        "def discretize_delta(delta: float) -> float:\n",
        "    idx = int(np.argmin(np.abs(delta_grid - delta)))\n",
        "    return float(delta_grid[idx])\n",
        "\n",
        "def epsilon(episode: int, total: int) -> float:\n",
        "    progress = episode / max(total - 1, 1)\n",
        "    return cfg.eps_start + progress * (cfg.eps_end - cfg.eps_start)\n",
        "\n",
        "def pick_action(s_idx: int, eps: float) -> int:\n",
        "    if rng.random() < eps:\n",
        "        return int(rng.integers(len(action_set)))\n",
        "    return int(np.argmax(q_table[s_idx]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "episode_returns: List[float] = []\n",
        "\n",
        "for ep in range(cfg.episodes):\n",
        "    state = env.reset(x=x, delta=0.0)\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "\n",
        "    while not done:\n",
        "        d_disc = discretize_delta(state.delta)\n",
        "        s_idx = state_index[(state.t, d_disc)]\n",
        "\n",
        "        a_idx = pick_action(s_idx, epsilon(ep, cfg.episodes))\n",
        "        action = float(action_set[a_idx])\n",
        "\n",
        "        step = env.step(action)\n",
        "        next_state = step.state\n",
        "        reward = step.reward\n",
        "        done = step.done\n",
        "\n",
        "        if done:\n",
        "            td_target = reward\n",
        "        else:\n",
        "            nd_disc = discretize_delta(next_state.delta)\n",
        "            ns_idx = state_index[(next_state.t, nd_disc)]\n",
        "            td_target = reward + cfg.gamma * np.max(q_table[ns_idx])\n",
        "\n",
        "        q_table[s_idx, a_idx] += cfg.alpha * (td_target - q_table[s_idx, a_idx])\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    episode_returns.append(total_reward)\n",
        "\n",
        "print(f\"Training complete. Mean reward (last 200 episodes): {np.mean(episode_returns[-200:]):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Quick diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "window = 100\n",
        "moving_avg = np.convolve(episode_returns, np.ones(window) / window, mode=\"valid\")\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(moving_avg)\n",
        "plt.title(\"Training reward (moving average)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Average return\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Run one greedy rollout (learned policy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state = env.reset(x=x, delta=0.0)\n",
        "done = False\n",
        "trace = []\n",
        "total_reward = 0.0\n",
        "\n",
        "while not done:\n",
        "    d_disc = discretize_delta(state.delta)\n",
        "    s_idx = state_index[(state.t, d_disc)]\n",
        "    a_idx = int(np.argmax(q_table[s_idx]))\n",
        "    action = float(action_set[a_idx])\n",
        "    result = env.step(action)\n",
        "\n",
        "    trace.append({\n",
        "        \"t\": state.t,\n",
        "        \"delta\": state.delta,\n",
        "        \"action\": action,\n",
        "        \"reward\": result.reward,\n",
        "        \"sold\": result.event,\n",
        "    })\n",
        "\n",
        "    total_reward += result.reward\n",
        "    state = result.state\n",
        "    done = result.done\n",
        "\n",
        "trace[:10], total_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You now have a full RL loop in one notebook: environment setup, training, and evaluation.\n",
        "To adapt this to production data, replace `hazard_fn` and `p0_fn` with model-backed functions and keep the training loop unchanged.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}